{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4470892f",
   "metadata": {},
   "source": [
    "# Wisconsin Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05b3ec",
   "metadata": {},
   "source": [
    "2/10/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6956e",
   "metadata": {},
   "source": [
    "Max Rivera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fdd10",
   "metadata": {},
   "source": [
    "Dataset from https://www.kaggle.com/uciml/breast-cancer-wisconsin-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e511187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c909ec8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8ef6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16b7dc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a6598",
   "metadata": {},
   "source": [
    "There is an unnamed column that needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27dc829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0    ...        25.380          17.33           184.60      2019.0   \n",
       "1    ...        24.990          23.41           158.80      1956.0   \n",
       "2    ...        23.570          25.53           152.50      1709.0   \n",
       "3    ...        14.910          26.50            98.87       567.7   \n",
       "4    ...        22.540          16.67           152.20      1575.0   \n",
       "..   ...           ...            ...              ...         ...   \n",
       "564  ...        25.450          26.40           166.10      2027.0   \n",
       "565  ...        23.690          38.25           155.00      1731.0   \n",
       "566  ...        18.980          34.12           126.70      1124.0   \n",
       "567  ...        25.740          39.42           184.60      1821.0   \n",
       "568  ...         9.456          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnamed column \n",
    "df = df.dropna(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd215c",
   "metadata": {},
   "source": [
    "The 'id' column doesn't provide any useful information about the tumor, so it needs to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a91030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           M        17.99         10.38          122.80     1001.0   \n",
       "1           M        20.57         17.77          132.90     1326.0   \n",
       "2           M        19.69         21.25          130.00     1203.0   \n",
       "3           M        11.42         20.38           77.58      386.1   \n",
       "4           M        20.29         14.34          135.10     1297.0   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564         M        21.56         22.39          142.00     1479.0   \n",
       "565         M        20.13         28.25          131.20     1261.0   \n",
       "566         M        16.60         28.08          108.30      858.1   \n",
       "567         M        20.60         29.33          140.10     1265.0   \n",
       "568         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove 'id' column\n",
    "df = df.drop('id',axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56ec74",
   "metadata": {},
   "source": [
    "For binary classification purposes, the 'diagnosis' column should be in the form of 0 or 1 instead of M or B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000021f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0            1        17.99         10.38          122.80     1001.0   \n",
       "1            1        20.57         17.77          132.90     1326.0   \n",
       "2            1        19.69         21.25          130.00     1203.0   \n",
       "3            1        11.42         20.38           77.58      386.1   \n",
       "4            1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564          1        21.56         22.39          142.00     1479.0   \n",
       "565          1        20.13         28.25          131.20     1261.0   \n",
       "566          1        16.60         28.08          108.30      858.1   \n",
       "567          1        20.60         29.33          140.10     1265.0   \n",
       "568          0         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diagnosis'] = np.array([1 if i == 'M' else 0 for i in df['diagnosis']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ca81b",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f34c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='diagnosis', ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='diagnosis',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20362469",
   "metadata": {},
   "source": [
    "The distribution of classes is good enough to use metrics such as classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1d68b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# correlation of each feature with the classes\n",
    "df.corr()['diagnosis'][1:].sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69fa7b",
   "metadata": {},
   "source": [
    "Most features are correlate with the output classes. There are a few features which do not. But for the first iteration of the model they can remain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed140e65",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d70f3d6",
   "metadata": {},
   "source": [
    "The train test split will be 70-30, since the dataset is pretty small (only 567 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c13fb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('diagnosis',axis=1).values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d2580f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['diagnosis'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da97a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000105a",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf71871",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b5d0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each feature is scaled between 0 and 1\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f75daa",
   "metadata": {},
   "source": [
    "The scaler is only fit on the training data, because we don't want to look at our test data at all or we won't have an unbiased evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cb0393d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.344445</td>\n",
       "      <td>0.325166</td>\n",
       "      <td>0.331635</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.474382</td>\n",
       "      <td>0.259045</td>\n",
       "      <td>0.208312</td>\n",
       "      <td>0.239318</td>\n",
       "      <td>0.380789</td>\n",
       "      <td>0.276509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292528</td>\n",
       "      <td>0.369829</td>\n",
       "      <td>0.279342</td>\n",
       "      <td>0.168791</td>\n",
       "      <td>0.406416</td>\n",
       "      <td>0.221953</td>\n",
       "      <td>0.220572</td>\n",
       "      <td>0.393164</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.195060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.175694</td>\n",
       "      <td>0.148126</td>\n",
       "      <td>0.172713</td>\n",
       "      <td>0.151680</td>\n",
       "      <td>0.152889</td>\n",
       "      <td>0.162484</td>\n",
       "      <td>0.192375</td>\n",
       "      <td>0.194286</td>\n",
       "      <td>0.137436</td>\n",
       "      <td>0.152144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176238</td>\n",
       "      <td>0.167026</td>\n",
       "      <td>0.171036</td>\n",
       "      <td>0.145152</td>\n",
       "      <td>0.150028</td>\n",
       "      <td>0.153445</td>\n",
       "      <td>0.173869</td>\n",
       "      <td>0.230207</td>\n",
       "      <td>0.123448</td>\n",
       "      <td>0.121759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.221708</td>\n",
       "      <td>0.215759</td>\n",
       "      <td>0.211446</td>\n",
       "      <td>0.111029</td>\n",
       "      <td>0.356169</td>\n",
       "      <td>0.138718</td>\n",
       "      <td>0.063894</td>\n",
       "      <td>0.097614</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.174126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173693</td>\n",
       "      <td>0.244203</td>\n",
       "      <td>0.162794</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>0.300337</td>\n",
       "      <td>0.112651</td>\n",
       "      <td>0.086861</td>\n",
       "      <td>0.219536</td>\n",
       "      <td>0.186120</td>\n",
       "      <td>0.114568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.304761</td>\n",
       "      <td>0.310619</td>\n",
       "      <td>0.293585</td>\n",
       "      <td>0.167678</td>\n",
       "      <td>0.473716</td>\n",
       "      <td>0.228989</td>\n",
       "      <td>0.140745</td>\n",
       "      <td>0.161556</td>\n",
       "      <td>0.369697</td>\n",
       "      <td>0.250527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245998</td>\n",
       "      <td>0.355943</td>\n",
       "      <td>0.232930</td>\n",
       "      <td>0.120367</td>\n",
       "      <td>0.405996</td>\n",
       "      <td>0.180177</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>0.335601</td>\n",
       "      <td>0.248127</td>\n",
       "      <td>0.167936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.427810</td>\n",
       "      <td>0.411481</td>\n",
       "      <td>0.417581</td>\n",
       "      <td>0.262185</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.340455</td>\n",
       "      <td>0.308341</td>\n",
       "      <td>0.361096</td>\n",
       "      <td>0.453662</td>\n",
       "      <td>0.347883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376201</td>\n",
       "      <td>0.486474</td>\n",
       "      <td>0.371358</td>\n",
       "      <td>0.210517</td>\n",
       "      <td>0.494651</td>\n",
       "      <td>0.308001</td>\n",
       "      <td>0.310144</td>\n",
       "      <td>0.567354</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.247948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  398.000000  398.000000  398.000000  398.000000  398.000000  398.000000   \n",
       "mean     0.344445    0.325166    0.331635    0.213454    0.474382    0.259045   \n",
       "std      0.175694    0.148126    0.172713    0.151680    0.152889    0.162484   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.221708    0.215759    0.211446    0.111029    0.356169    0.138718   \n",
       "50%      0.304761    0.310619    0.293585    0.167678    0.473716    0.228989   \n",
       "75%      0.427810    0.411481    0.417581    0.262185    0.569892    0.340455   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               6           7           8           9   ...          20  \\\n",
       "count  398.000000  398.000000  398.000000  398.000000  ...  398.000000   \n",
       "mean     0.208312    0.239318    0.380789    0.276509  ...    0.292528   \n",
       "std      0.192375    0.194286    0.137436    0.152144  ...    0.176238   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.063894    0.097614    0.288889    0.174126  ...    0.173693   \n",
       "50%      0.140745    0.161556    0.369697    0.250527  ...    0.245998   \n",
       "75%      0.308341    0.361096    0.453662    0.347883  ...    0.376201   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "               21          22          23          24          25          26  \\\n",
       "count  398.000000  398.000000  398.000000  398.000000  398.000000  398.000000   \n",
       "mean     0.369829    0.279342    0.168791    0.406416    0.221953    0.220572   \n",
       "std      0.167026    0.171036    0.145152    0.150028    0.153445    0.173869   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.244203    0.162794    0.076251    0.300337    0.112651    0.086861   \n",
       "50%      0.355943    0.232930    0.120367    0.405996    0.180177    0.179233   \n",
       "75%      0.486474    0.371358    0.210517    0.494651    0.308001    0.310144   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               27          28          29  \n",
       "count  398.000000  398.000000  398.000000  \n",
       "mean     0.393164    0.265500    0.195060  \n",
       "std      0.230207    0.123448    0.121759  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.219536    0.186120    0.114568  \n",
       "50%      0.335601    0.248127    0.167936  \n",
       "75%      0.567354    0.316000    0.247948  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show statistics for X_train\n",
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d156d4c",
   "metadata": {},
   "source": [
    "Each feature of X_train is normalized so that the max value is 1 and the min value is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b54f60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7abb1c1",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3bfd7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dense(15,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657072c2",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4107b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 14:20:48.037141: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-10 14:20:48.037588: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples, validate on 171 samples\n",
      "Epoch 1/600\n",
      "398/398 [==============================] - 1s 2ms/sample - loss: 0.6782 - val_loss: 0.6637\n",
      "Epoch 2/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.6464 - val_loss: 0.6361\n",
      "Epoch 3/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.6118 - val_loss: 0.6039\n",
      "Epoch 4/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.5723 - val_loss: 0.5644\n",
      "Epoch 5/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.5282 - val_loss: 0.5184\n",
      "Epoch 6/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.4792 - val_loss: 0.4710\n",
      "Epoch 7/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.4325 - val_loss: 0.4248\n",
      "Epoch 8/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.3860 - val_loss: 0.3824\n",
      "Epoch 9/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.3428 - val_loss: 0.3456\n",
      "Epoch 10/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.3101 - val_loss: 0.3117\n",
      "Epoch 11/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.2771 - val_loss: 0.2840\n",
      "Epoch 12/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.2492 - val_loss: 0.2597\n",
      "Epoch 13/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.2255 - val_loss: 0.2422\n",
      "Epoch 14/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.2067 - val_loss: 0.2263\n",
      "Epoch 15/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1909 - val_loss: 0.2144\n",
      "Epoch 16/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1776 - val_loss: 0.2033\n",
      "Epoch 17/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1673 - val_loss: 0.1941\n",
      "Epoch 18/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1569 - val_loss: 0.1878\n",
      "Epoch 19/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1476 - val_loss: 0.1801\n",
      "Epoch 20/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1382 - val_loss: 0.1746\n",
      "Epoch 21/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.1302 - val_loss: 0.1693\n",
      "Epoch 22/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1234 - val_loss: 0.1658\n",
      "Epoch 23/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1183 - val_loss: 0.1615\n",
      "Epoch 24/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1109 - val_loss: 0.1571\n",
      "Epoch 25/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.1064 - val_loss: 0.1544\n",
      "Epoch 26/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1034 - val_loss: 0.1513\n",
      "Epoch 27/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0966 - val_loss: 0.1474\n",
      "Epoch 28/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.0935 - val_loss: 0.1443\n",
      "Epoch 29/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0892 - val_loss: 0.1421\n",
      "Epoch 30/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0847 - val_loss: 0.1412\n",
      "Epoch 31/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0824 - val_loss: 0.1399\n",
      "Epoch 32/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0829 - val_loss: 0.1382\n",
      "Epoch 33/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0825 - val_loss: 0.1363\n",
      "Epoch 34/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0767 - val_loss: 0.1344\n",
      "Epoch 35/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0724 - val_loss: 0.1329\n",
      "Epoch 36/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0707 - val_loss: 0.1318\n",
      "Epoch 37/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0677 - val_loss: 0.1306\n",
      "Epoch 38/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0665 - val_loss: 0.1289\n",
      "Epoch 39/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0657 - val_loss: 0.1301\n",
      "Epoch 40/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0635 - val_loss: 0.1278\n",
      "Epoch 41/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0638 - val_loss: 0.1275\n",
      "Epoch 42/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0610 - val_loss: 0.1273\n",
      "Epoch 43/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0595 - val_loss: 0.1259\n",
      "Epoch 44/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0581 - val_loss: 0.1252\n",
      "Epoch 45/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0566 - val_loss: 0.1247\n",
      "Epoch 46/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0557 - val_loss: 0.1244\n",
      "Epoch 47/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0552 - val_loss: 0.1240\n",
      "Epoch 48/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0538 - val_loss: 0.1250\n",
      "Epoch 49/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0540 - val_loss: 0.1227\n",
      "Epoch 50/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0567 - val_loss: 0.1230\n",
      "Epoch 51/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0530 - val_loss: 0.1261\n",
      "Epoch 52/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0518 - val_loss: 0.1220\n",
      "Epoch 53/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0497 - val_loss: 0.1242\n",
      "Epoch 54/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0492 - val_loss: 0.1224\n",
      "Epoch 55/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0485 - val_loss: 0.1230\n",
      "Epoch 56/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0492 - val_loss: 0.1234\n",
      "Epoch 57/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0480 - val_loss: 0.1234\n",
      "Epoch 58/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0474 - val_loss: 0.1227\n",
      "Epoch 59/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0483 - val_loss: 0.1225\n",
      "Epoch 60/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0493 - val_loss: 0.1237\n",
      "Epoch 61/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0509 - val_loss: 0.1219\n",
      "Epoch 62/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0571 - val_loss: 0.1225\n",
      "Epoch 63/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0519 - val_loss: 0.1217\n",
      "Epoch 64/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0503 - val_loss: 0.1216\n",
      "Epoch 65/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0451 - val_loss: 0.1225\n",
      "Epoch 66/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0441 - val_loss: 0.1223\n",
      "Epoch 67/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0463 - val_loss: 0.1228\n",
      "Epoch 68/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0435 - val_loss: 0.1242\n",
      "Epoch 69/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0418 - val_loss: 0.1232\n",
      "Epoch 70/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0432 - val_loss: 0.1243\n",
      "Epoch 71/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0434 - val_loss: 0.1243\n",
      "Epoch 72/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0419 - val_loss: 0.1229\n",
      "Epoch 73/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0436 - val_loss: 0.1245\n",
      "Epoch 74/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0406 - val_loss: 0.1243\n",
      "Epoch 75/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0409 - val_loss: 0.1289\n",
      "Epoch 76/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0408 - val_loss: 0.1236\n",
      "Epoch 77/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0393 - val_loss: 0.1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0389 - val_loss: 0.1255\n",
      "Epoch 79/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0395 - val_loss: 0.1252\n",
      "Epoch 80/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0388 - val_loss: 0.1250\n",
      "Epoch 81/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0389 - val_loss: 0.1240\n",
      "Epoch 82/600\n",
      "398/398 [==============================] - 0s 98us/sample - loss: 0.0391 - val_loss: 0.1256\n",
      "Epoch 83/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0379 - val_loss: 0.1252\n",
      "Epoch 84/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0402 - val_loss: 0.1251\n",
      "Epoch 85/600\n",
      "398/398 [==============================] - 0s 98us/sample - loss: 0.0378 - val_loss: 0.1259\n",
      "Epoch 86/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0395 - val_loss: 0.1256\n",
      "Epoch 87/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0373 - val_loss: 0.1253\n",
      "Epoch 88/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0368 - val_loss: 0.1277\n",
      "Epoch 89/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0366 - val_loss: 0.1253\n",
      "Epoch 90/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0382 - val_loss: 0.1264\n",
      "Epoch 91/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0407 - val_loss: 0.1290\n",
      "Epoch 92/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0393 - val_loss: 0.1247\n",
      "Epoch 93/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0434 - val_loss: 0.1290\n",
      "Epoch 94/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0381 - val_loss: 0.1261\n",
      "Epoch 95/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0349 - val_loss: 0.1254\n",
      "Epoch 96/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0349 - val_loss: 0.1253\n",
      "Epoch 97/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0344 - val_loss: 0.1275\n",
      "Epoch 98/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0373 - val_loss: 0.1247\n",
      "Epoch 99/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0363 - val_loss: 0.1251\n",
      "Epoch 100/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0342 - val_loss: 0.1266\n",
      "Epoch 101/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0341 - val_loss: 0.1263\n",
      "Epoch 102/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0374 - val_loss: 0.1253\n",
      "Epoch 103/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0373 - val_loss: 0.1276\n",
      "Epoch 104/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0346 - val_loss: 0.1254\n",
      "Epoch 105/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0330 - val_loss: 0.1262\n",
      "Epoch 106/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0330 - val_loss: 0.1276\n",
      "Epoch 107/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0330 - val_loss: 0.1268\n",
      "Epoch 108/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0368 - val_loss: 0.1273\n",
      "Epoch 109/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0346 - val_loss: 0.1265\n",
      "Epoch 110/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0453 - val_loss: 0.1279\n",
      "Epoch 111/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0352 - val_loss: 0.1279\n",
      "Epoch 112/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0326 - val_loss: 0.1285\n",
      "Epoch 113/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0350 - val_loss: 0.1269\n",
      "Epoch 114/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0324 - val_loss: 0.1284\n",
      "Epoch 115/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0339 - val_loss: 0.1262\n",
      "Epoch 116/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0335 - val_loss: 0.1265\n",
      "Epoch 117/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0345 - val_loss: 0.1278\n",
      "Epoch 118/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.0350 - val_loss: 0.1271\n",
      "Epoch 119/600\n",
      "398/398 [==============================] - 0s 118us/sample - loss: 0.0342 - val_loss: 0.1257\n",
      "Epoch 120/600\n",
      "398/398 [==============================] - 0s 115us/sample - loss: 0.0315 - val_loss: 0.1310\n",
      "Epoch 121/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0337 - val_loss: 0.1279\n",
      "Epoch 122/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0325 - val_loss: 0.1289\n",
      "Epoch 123/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0302 - val_loss: 0.1283\n",
      "Epoch 124/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0295 - val_loss: 0.1261\n",
      "Epoch 125/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0298 - val_loss: 0.1324\n",
      "Epoch 126/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0319 - val_loss: 0.1274\n",
      "Epoch 127/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0296 - val_loss: 0.1277\n",
      "Epoch 128/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0328 - val_loss: 0.1363\n",
      "Epoch 129/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0368 - val_loss: 0.1271\n",
      "Epoch 130/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0302 - val_loss: 0.1278\n",
      "Epoch 131/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0319 - val_loss: 0.1290\n",
      "Epoch 132/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0334 - val_loss: 0.1357\n",
      "Epoch 133/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0310 - val_loss: 0.1300\n",
      "Epoch 134/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0311 - val_loss: 0.1293\n",
      "Epoch 135/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0340 - val_loss: 0.1316\n",
      "Epoch 136/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0325 - val_loss: 0.1328\n",
      "Epoch 137/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0323 - val_loss: 0.1337\n",
      "Epoch 138/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0277 - val_loss: 0.1310\n",
      "Epoch 139/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0298 - val_loss: 0.1336\n",
      "Epoch 140/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0278 - val_loss: 0.1341\n",
      "Epoch 141/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0263 - val_loss: 0.1306\n",
      "Epoch 142/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0272 - val_loss: 0.1324\n",
      "Epoch 143/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0288 - val_loss: 0.1324\n",
      "Epoch 144/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0291 - val_loss: 0.1352\n",
      "Epoch 145/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0275 - val_loss: 0.1322\n",
      "Epoch 146/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0266 - val_loss: 0.1330\n",
      "Epoch 147/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0266 - val_loss: 0.1328\n",
      "Epoch 148/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0269 - val_loss: 0.1378\n",
      "Epoch 149/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0315 - val_loss: 0.1327\n",
      "Epoch 150/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0322 - val_loss: 0.1311\n",
      "Epoch 151/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0273 - val_loss: 0.1331\n",
      "Epoch 152/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0254 - val_loss: 0.1323\n",
      "Epoch 153/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0250 - val_loss: 0.1319\n",
      "Epoch 154/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0257 - val_loss: 0.1338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0252 - val_loss: 0.1335\n",
      "Epoch 156/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0271 - val_loss: 0.1353\n",
      "Epoch 157/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0251 - val_loss: 0.1355\n",
      "Epoch 158/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0247 - val_loss: 0.1329\n",
      "Epoch 159/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0259 - val_loss: 0.1356\n",
      "Epoch 160/600\n",
      "398/398 [==============================] - 0s 98us/sample - loss: 0.0250 - val_loss: 0.1369\n",
      "Epoch 161/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0248 - val_loss: 0.1356\n",
      "Epoch 162/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0246 - val_loss: 0.1369\n",
      "Epoch 163/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0244 - val_loss: 0.1376\n",
      "Epoch 164/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0259 - val_loss: 0.1362\n",
      "Epoch 165/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0248 - val_loss: 0.1367\n",
      "Epoch 166/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0281 - val_loss: 0.1370\n",
      "Epoch 167/600\n",
      "398/398 [==============================] - 0s 98us/sample - loss: 0.0274 - val_loss: 0.1371\n",
      "Epoch 168/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0253 - val_loss: 0.1367\n",
      "Epoch 169/600\n",
      "398/398 [==============================] - 0s 98us/sample - loss: 0.0234 - val_loss: 0.1375\n",
      "Epoch 170/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0236 - val_loss: 0.1364\n",
      "Epoch 171/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0227 - val_loss: 0.1367\n",
      "Epoch 172/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0229 - val_loss: 0.1382\n",
      "Epoch 173/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0229 - val_loss: 0.1383\n",
      "Epoch 174/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0273 - val_loss: 0.1370\n",
      "Epoch 175/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0246 - val_loss: 0.1397\n",
      "Epoch 176/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0258 - val_loss: 0.1379\n",
      "Epoch 177/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0362 - val_loss: 0.1380\n",
      "Epoch 178/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0306 - val_loss: 0.1379\n",
      "Epoch 179/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0275 - val_loss: 0.1364\n",
      "Epoch 180/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0226 - val_loss: 0.1415\n",
      "Epoch 181/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0244 - val_loss: 0.1353\n",
      "Epoch 182/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0275 - val_loss: 0.1403\n",
      "Epoch 183/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0273 - val_loss: 0.1380\n",
      "Epoch 184/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0234 - val_loss: 0.1368\n",
      "Epoch 185/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0226 - val_loss: 0.1384\n",
      "Epoch 186/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0230 - val_loss: 0.1370\n",
      "Epoch 187/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0220 - val_loss: 0.1359\n",
      "Epoch 188/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0205 - val_loss: 0.1398\n",
      "Epoch 189/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0232 - val_loss: 0.1364\n",
      "Epoch 190/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0244 - val_loss: 0.1405\n",
      "Epoch 191/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0287 - val_loss: 0.1389\n",
      "Epoch 192/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0269 - val_loss: 0.1465\n",
      "Epoch 193/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0241 - val_loss: 0.1406\n",
      "Epoch 194/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0194 - val_loss: 0.1478\n",
      "Epoch 195/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0208 - val_loss: 0.1400\n",
      "Epoch 196/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0220 - val_loss: 0.1449\n",
      "Epoch 197/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0192 - val_loss: 0.1417\n",
      "Epoch 198/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0202 - val_loss: 0.1415\n",
      "Epoch 199/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0194 - val_loss: 0.1409\n",
      "Epoch 200/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0192 - val_loss: 0.1432\n",
      "Epoch 201/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0192 - val_loss: 0.1412\n",
      "Epoch 202/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0190 - val_loss: 0.1407\n",
      "Epoch 203/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0188 - val_loss: 0.1430\n",
      "Epoch 204/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0196 - val_loss: 0.1401\n",
      "Epoch 205/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0189 - val_loss: 0.1439\n",
      "Epoch 206/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0191 - val_loss: 0.1421\n",
      "Epoch 207/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0184 - val_loss: 0.1404\n",
      "Epoch 208/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0191 - val_loss: 0.1437\n",
      "Epoch 209/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0186 - val_loss: 0.1408\n",
      "Epoch 210/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0198 - val_loss: 0.1425\n",
      "Epoch 211/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0187 - val_loss: 0.1391\n",
      "Epoch 212/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0197 - val_loss: 0.1395\n",
      "Epoch 213/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0214 - val_loss: 0.1462\n",
      "Epoch 214/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0198 - val_loss: 0.1401\n",
      "Epoch 215/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0301 - val_loss: 0.1512\n",
      "Epoch 216/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0201 - val_loss: 0.1410\n",
      "Epoch 217/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0211 - val_loss: 0.1485\n",
      "Epoch 218/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0173 - val_loss: 0.1408\n",
      "Epoch 219/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0181 - val_loss: 0.1424\n",
      "Epoch 220/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0173 - val_loss: 0.1424\n",
      "Epoch 221/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0178 - val_loss: 0.1418\n",
      "Epoch 222/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0174 - val_loss: 0.1454\n",
      "Epoch 223/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0187 - val_loss: 0.1472\n",
      "Epoch 224/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0168 - val_loss: 0.1422\n",
      "Epoch 225/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0180 - val_loss: 0.1436\n",
      "Epoch 226/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0163 - val_loss: 0.1424\n",
      "Epoch 227/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0167 - val_loss: 0.1457\n",
      "Epoch 228/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0178 - val_loss: 0.1425\n",
      "Epoch 229/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0171 - val_loss: 0.1428\n",
      "Epoch 230/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0161 - val_loss: 0.1424\n",
      "Epoch 231/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0171 - val_loss: 0.1451\n",
      "Epoch 232/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0168 - val_loss: 0.1443\n",
      "Epoch 233/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0168 - val_loss: 0.1431\n",
      "Epoch 234/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0169 - val_loss: 0.1519\n",
      "Epoch 235/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0160 - val_loss: 0.1463\n",
      "Epoch 236/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0191 - val_loss: 0.1527\n",
      "Epoch 237/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0200 - val_loss: 0.1453\n",
      "Epoch 238/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0160 - val_loss: 0.1460\n",
      "Epoch 239/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0149 - val_loss: 0.1458\n",
      "Epoch 240/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0209 - val_loss: 0.1492\n",
      "Epoch 241/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0168 - val_loss: 0.1513\n",
      "Epoch 242/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0144 - val_loss: 0.1450\n",
      "Epoch 243/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0155 - val_loss: 0.1460\n",
      "Epoch 244/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0143 - val_loss: 0.1450\n",
      "Epoch 245/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0149 - val_loss: 0.1482\n",
      "Epoch 246/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0150 - val_loss: 0.1500\n",
      "Epoch 247/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0139 - val_loss: 0.1465\n",
      "Epoch 248/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0171 - val_loss: 0.1497\n",
      "Epoch 249/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0159 - val_loss: 0.1474\n",
      "Epoch 250/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0144 - val_loss: 0.1507\n",
      "Epoch 251/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0143 - val_loss: 0.1488\n",
      "Epoch 252/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0150 - val_loss: 0.1490\n",
      "Epoch 253/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0148 - val_loss: 0.1506\n",
      "Epoch 254/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0147 - val_loss: 0.1503\n",
      "Epoch 255/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0139 - val_loss: 0.1497\n",
      "Epoch 256/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0142 - val_loss: 0.1536\n",
      "Epoch 257/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0137 - val_loss: 0.1491\n",
      "Epoch 258/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0141 - val_loss: 0.1511\n",
      "Epoch 259/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0131 - val_loss: 0.1511\n",
      "Epoch 260/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.0152 - val_loss: 0.1518\n",
      "Epoch 261/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0197 - val_loss: 0.1519\n",
      "Epoch 262/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0152 - val_loss: 0.1527\n",
      "Epoch 263/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0133 - val_loss: 0.1517\n",
      "Epoch 264/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0126 - val_loss: 0.1515\n",
      "Epoch 265/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0126 - val_loss: 0.1523\n",
      "Epoch 266/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0125 - val_loss: 0.1530\n",
      "Epoch 267/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0128 - val_loss: 0.1555\n",
      "Epoch 268/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0126 - val_loss: 0.1531\n",
      "Epoch 269/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0125 - val_loss: 0.1533\n",
      "Epoch 270/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0144 - val_loss: 0.1581\n",
      "Epoch 271/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0136 - val_loss: 0.1570\n",
      "Epoch 272/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0147 - val_loss: 0.1569\n",
      "Epoch 273/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0148 - val_loss: 0.1506\n",
      "Epoch 274/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0133 - val_loss: 0.1526\n",
      "Epoch 275/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0128 - val_loss: 0.1518\n",
      "Epoch 276/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0127 - val_loss: 0.1545\n",
      "Epoch 277/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0132 - val_loss: 0.1585\n",
      "Epoch 278/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0119 - val_loss: 0.1541\n",
      "Epoch 279/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0155 - val_loss: 0.1548\n",
      "Epoch 280/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0130 - val_loss: 0.1543\n",
      "Epoch 281/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0128 - val_loss: 0.1571\n",
      "Epoch 282/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0123 - val_loss: 0.1556\n",
      "Epoch 283/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0142 - val_loss: 0.1631\n",
      "Epoch 284/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0141 - val_loss: 0.1579\n",
      "Epoch 285/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0124 - val_loss: 0.1620\n",
      "Epoch 286/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0112 - val_loss: 0.1591\n",
      "Epoch 287/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0113 - val_loss: 0.1601\n",
      "Epoch 288/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0110 - val_loss: 0.1593\n",
      "Epoch 289/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0106 - val_loss: 0.1598\n",
      "Epoch 290/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0114 - val_loss: 0.1607\n",
      "Epoch 291/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0105 - val_loss: 0.1622\n",
      "Epoch 292/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0116 - val_loss: 0.1620\n",
      "Epoch 293/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0112 - val_loss: 0.1624\n",
      "Epoch 294/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0109 - val_loss: 0.1599\n",
      "Epoch 295/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0109 - val_loss: 0.1612\n",
      "Epoch 296/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0109 - val_loss: 0.1641\n",
      "Epoch 297/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0110 - val_loss: 0.1637\n",
      "Epoch 298/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0110 - val_loss: 0.1644\n",
      "Epoch 299/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0099 - val_loss: 0.1663\n",
      "Epoch 300/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0105 - val_loss: 0.1680\n",
      "Epoch 301/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0121 - val_loss: 0.1691\n",
      "Epoch 302/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0101 - val_loss: 0.1680\n",
      "Epoch 303/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0101 - val_loss: 0.1693\n",
      "Epoch 304/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0099 - val_loss: 0.1695\n",
      "Epoch 305/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0095 - val_loss: 0.1677\n",
      "Epoch 306/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0093 - val_loss: 0.1680\n",
      "Epoch 307/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0096 - val_loss: 0.1702\n",
      "Epoch 308/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0105 - val_loss: 0.1695\n",
      "Epoch 309/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0108 - val_loss: 0.1725\n",
      "Epoch 310/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0114 - val_loss: 0.1724\n",
      "Epoch 311/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0159 - val_loss: 0.1847\n",
      "Epoch 312/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0147 - val_loss: 0.1791\n",
      "Epoch 313/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0136 - val_loss: 0.1652\n",
      "Epoch 314/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0094 - val_loss: 0.1650\n",
      "Epoch 315/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0088 - val_loss: 0.1669\n",
      "Epoch 316/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0088 - val_loss: 0.1658\n",
      "Epoch 317/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0097 - val_loss: 0.1665\n",
      "Epoch 318/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0092 - val_loss: 0.1693\n",
      "Epoch 319/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0088 - val_loss: 0.1706\n",
      "Epoch 320/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0099 - val_loss: 0.1733\n",
      "Epoch 321/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0106 - val_loss: 0.1765\n",
      "Epoch 322/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0121 - val_loss: 0.1756\n",
      "Epoch 323/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0086 - val_loss: 0.1742\n",
      "Epoch 324/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0083 - val_loss: 0.1740\n",
      "Epoch 325/600\n",
      "398/398 [==============================] - 0s 126us/sample - loss: 0.0088 - val_loss: 0.1730\n",
      "Epoch 326/600\n",
      "398/398 [==============================] - 0s 174us/sample - loss: 0.0104 - val_loss: 0.1864\n",
      "Epoch 327/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0101 - val_loss: 0.1727\n",
      "Epoch 328/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0123 - val_loss: 0.1812\n",
      "Epoch 329/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.0086 - val_loss: 0.1765\n",
      "Epoch 330/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0096 - val_loss: 0.1797\n",
      "Epoch 331/600\n",
      "398/398 [==============================] - 0s 134us/sample - loss: 0.0103 - val_loss: 0.1814\n",
      "Epoch 332/600\n",
      "398/398 [==============================] - 0s 122us/sample - loss: 0.0114 - val_loss: 0.1763\n",
      "Epoch 333/600\n",
      "398/398 [==============================] - 0s 128us/sample - loss: 0.0096 - val_loss: 0.1762\n",
      "Epoch 334/600\n",
      "398/398 [==============================] - 0s 117us/sample - loss: 0.0087 - val_loss: 0.1764\n",
      "Epoch 335/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0084 - val_loss: 0.1784\n",
      "Epoch 336/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0111 - val_loss: 0.1803\n",
      "Epoch 337/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.0095 - val_loss: 0.1775\n",
      "Epoch 338/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0086 - val_loss: 0.1773\n",
      "Epoch 339/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0078 - val_loss: 0.1780\n",
      "Epoch 340/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0077 - val_loss: 0.1785\n",
      "Epoch 341/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0075 - val_loss: 0.1783\n",
      "Epoch 342/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0075 - val_loss: 0.1805\n",
      "Epoch 343/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0072 - val_loss: 0.1820\n",
      "Epoch 344/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0074 - val_loss: 0.1810\n",
      "Epoch 345/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0071 - val_loss: 0.1819\n",
      "Epoch 346/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0074 - val_loss: 0.1829\n",
      "Epoch 347/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0070 - val_loss: 0.1844\n",
      "Epoch 348/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0084 - val_loss: 0.1859\n",
      "Epoch 349/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0132 - val_loss: 0.1945\n",
      "Epoch 350/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0099 - val_loss: 0.1870\n",
      "Epoch 351/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0078 - val_loss: 0.1854\n",
      "Epoch 352/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0077 - val_loss: 0.1892\n",
      "Epoch 353/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0078 - val_loss: 0.1865\n",
      "Epoch 354/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0069 - val_loss: 0.1887\n",
      "Epoch 355/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0068 - val_loss: 0.1916\n",
      "Epoch 356/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0069 - val_loss: 0.1923\n",
      "Epoch 357/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0069 - val_loss: 0.1940\n",
      "Epoch 358/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0071 - val_loss: 0.1958\n",
      "Epoch 359/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0077 - val_loss: 0.1952\n",
      "Epoch 360/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0091 - val_loss: 0.1950\n",
      "Epoch 361/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0083 - val_loss: 0.2010\n",
      "Epoch 362/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0095 - val_loss: 0.1974\n",
      "Epoch 363/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0085 - val_loss: 0.1961\n",
      "Epoch 364/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0061 - val_loss: 0.1929\n",
      "Epoch 365/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0067 - val_loss: 0.1935\n",
      "Epoch 366/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0069 - val_loss: 0.1919\n",
      "Epoch 367/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0070 - val_loss: 0.1951\n",
      "Epoch 368/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0062 - val_loss: 0.1971\n",
      "Epoch 369/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0061 - val_loss: 0.1972\n",
      "Epoch 370/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0062 - val_loss: 0.1978\n",
      "Epoch 371/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0061 - val_loss: 0.1972\n",
      "Epoch 372/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0069 - val_loss: 0.1991\n",
      "Epoch 373/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0064 - val_loss: 0.1962\n",
      "Epoch 374/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0059 - val_loss: 0.1965\n",
      "Epoch 375/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0060 - val_loss: 0.1952\n",
      "Epoch 376/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0061 - val_loss: 0.1976\n",
      "Epoch 377/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0065 - val_loss: 0.1965\n",
      "Epoch 378/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0059 - val_loss: 0.2013\n",
      "Epoch 379/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0077 - val_loss: 0.2005\n",
      "Epoch 380/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0066 - val_loss: 0.2052\n",
      "Epoch 381/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0068 - val_loss: 0.2025\n",
      "Epoch 382/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0055 - val_loss: 0.2043\n",
      "Epoch 383/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0056 - val_loss: 0.2023\n",
      "Epoch 384/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0060 - val_loss: 0.2040\n",
      "Epoch 385/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0056 - val_loss: 0.2059\n",
      "Epoch 386/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0059 - val_loss: 0.2042\n",
      "Epoch 387/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0051 - val_loss: 0.2109\n",
      "Epoch 388/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0070 - val_loss: 0.2062\n",
      "Epoch 389/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0053 - val_loss: 0.2063\n",
      "Epoch 390/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0054 - val_loss: 0.2064\n",
      "Epoch 391/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0055 - val_loss: 0.2088\n",
      "Epoch 392/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0050 - val_loss: 0.2121\n",
      "Epoch 393/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0053 - val_loss: 0.2122\n",
      "Epoch 394/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0057 - val_loss: 0.2120\n",
      "Epoch 395/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0073 - val_loss: 0.2099\n",
      "Epoch 396/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0087 - val_loss: 0.2166\n",
      "Epoch 397/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0059 - val_loss: 0.2099\n",
      "Epoch 398/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0051 - val_loss: 0.2130\n",
      "Epoch 399/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0051 - val_loss: 0.2115\n",
      "Epoch 400/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0049 - val_loss: 0.2132\n",
      "Epoch 401/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0047 - val_loss: 0.2152\n",
      "Epoch 402/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0052 - val_loss: 0.2128\n",
      "Epoch 403/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0058 - val_loss: 0.2190\n",
      "Epoch 404/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0059 - val_loss: 0.2137\n",
      "Epoch 405/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0044 - val_loss: 0.2186\n",
      "Epoch 406/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0050 - val_loss: 0.2155\n",
      "Epoch 407/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0056 - val_loss: 0.2292\n",
      "Epoch 408/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0081 - val_loss: 0.2252\n",
      "Epoch 409/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0064 - val_loss: 0.2297\n",
      "Epoch 410/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0045 - val_loss: 0.2200\n",
      "Epoch 411/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0059 - val_loss: 0.2249\n",
      "Epoch 412/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0063 - val_loss: 0.2154\n",
      "Epoch 413/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0047 - val_loss: 0.2192\n",
      "Epoch 414/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0047 - val_loss: 0.2182\n",
      "Epoch 415/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0047 - val_loss: 0.2210\n",
      "Epoch 416/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0050 - val_loss: 0.2190\n",
      "Epoch 417/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0049 - val_loss: 0.2203\n",
      "Epoch 418/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0074 - val_loss: 0.2311\n",
      "Epoch 419/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0061 - val_loss: 0.2233\n",
      "Epoch 420/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0057 - val_loss: 0.2287\n",
      "Epoch 421/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0055 - val_loss: 0.2202\n",
      "Epoch 422/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0050 - val_loss: 0.2217\n",
      "Epoch 423/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0047 - val_loss: 0.2205\n",
      "Epoch 424/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0041 - val_loss: 0.2255\n",
      "Epoch 425/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0050 - val_loss: 0.2218\n",
      "Epoch 426/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0040 - val_loss: 0.2267\n",
      "Epoch 427/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0044 - val_loss: 0.2253\n",
      "Epoch 428/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0041 - val_loss: 0.2274\n",
      "Epoch 429/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0039 - val_loss: 0.2364\n",
      "Epoch 430/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0045 - val_loss: 0.2294\n",
      "Epoch 431/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0042 - val_loss: 0.2338\n",
      "Epoch 432/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0040 - val_loss: 0.2317\n",
      "Epoch 433/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0042 - val_loss: 0.2345\n",
      "Epoch 434/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0039 - val_loss: 0.2338\n",
      "Epoch 435/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0040 - val_loss: 0.2363\n",
      "Epoch 436/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0037 - val_loss: 0.2357\n",
      "Epoch 437/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0038 - val_loss: 0.2380\n",
      "Epoch 438/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0046 - val_loss: 0.2334\n",
      "Epoch 439/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0039 - val_loss: 0.2368\n",
      "Epoch 440/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0048 - val_loss: 0.2333\n",
      "Epoch 441/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0054 - val_loss: 0.2429\n",
      "Epoch 442/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0047 - val_loss: 0.2363\n",
      "Epoch 443/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0039 - val_loss: 0.2363\n",
      "Epoch 444/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0037 - val_loss: 0.2352\n",
      "Epoch 445/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0036 - val_loss: 0.2380\n",
      "Epoch 446/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0036 - val_loss: 0.2372\n",
      "Epoch 447/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0037 - val_loss: 0.2401\n",
      "Epoch 448/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0034 - val_loss: 0.2382\n",
      "Epoch 449/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0038 - val_loss: 0.2400\n",
      "Epoch 450/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0037 - val_loss: 0.2404\n",
      "Epoch 451/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0038 - val_loss: 0.2404\n",
      "Epoch 452/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0040 - val_loss: 0.2434\n",
      "Epoch 453/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.0039 - val_loss: 0.2415\n",
      "Epoch 454/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0040 - val_loss: 0.2437\n",
      "Epoch 455/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0038 - val_loss: 0.2474\n",
      "Epoch 456/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0036 - val_loss: 0.2440\n",
      "Epoch 457/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0035 - val_loss: 0.2513\n",
      "Epoch 458/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0051 - val_loss: 0.2468\n",
      "Epoch 459/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0055 - val_loss: 0.2548\n",
      "Epoch 460/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0046 - val_loss: 0.2467\n",
      "Epoch 461/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0033 - val_loss: 0.2578\n",
      "Epoch 462/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0036 - val_loss: 0.2539\n",
      "Epoch 463/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0033 - val_loss: 0.2553\n",
      "Epoch 464/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0040 - val_loss: 0.2520\n",
      "Epoch 465/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0032 - val_loss: 0.2563\n",
      "Epoch 466/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0032 - val_loss: 0.2525\n",
      "Epoch 467/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0033 - val_loss: 0.2514\n",
      "Epoch 468/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0034 - val_loss: 0.2509\n",
      "Epoch 469/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0033 - val_loss: 0.2646\n",
      "Epoch 470/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0041 - val_loss: 0.2513\n",
      "Epoch 471/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0058 - val_loss: 0.2680\n",
      "Epoch 472/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0046 - val_loss: 0.2530\n",
      "Epoch 473/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0045 - val_loss: 0.2525\n",
      "Epoch 474/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0030 - val_loss: 0.2491\n",
      "Epoch 475/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0029 - val_loss: 0.2528\n",
      "Epoch 476/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0031 - val_loss: 0.2542\n",
      "Epoch 477/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0038 - val_loss: 0.2568\n",
      "Epoch 478/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0035 - val_loss: 0.2731\n",
      "Epoch 479/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0054 - val_loss: 0.2598\n",
      "Epoch 480/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0034 - val_loss: 0.2654\n",
      "Epoch 481/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0035 - val_loss: 0.2555\n",
      "Epoch 482/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0028 - val_loss: 0.2622\n",
      "Epoch 483/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0027 - val_loss: 0.2575\n",
      "Epoch 484/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0027 - val_loss: 0.2620\n",
      "Epoch 485/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0031 - val_loss: 0.2599\n",
      "Epoch 486/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0035 - val_loss: 0.2621\n",
      "Epoch 487/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0027 - val_loss: 0.2639\n",
      "Epoch 488/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0027 - val_loss: 0.2621\n",
      "Epoch 489/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0024 - val_loss: 0.2679\n",
      "Epoch 490/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0028 - val_loss: 0.2698\n",
      "Epoch 491/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0028 - val_loss: 0.2692\n",
      "Epoch 492/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0029 - val_loss: 0.2665\n",
      "Epoch 493/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0038 - val_loss: 0.2760\n",
      "Epoch 494/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0039 - val_loss: 0.2642\n",
      "Epoch 495/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0040 - val_loss: 0.2740\n",
      "Epoch 496/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0034 - val_loss: 0.2638\n",
      "Epoch 497/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0023 - val_loss: 0.2706\n",
      "Epoch 498/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0027 - val_loss: 0.2669\n",
      "Epoch 499/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0030 - val_loss: 0.2702\n",
      "Epoch 500/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0024 - val_loss: 0.2712\n",
      "Epoch 501/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0026 - val_loss: 0.2699\n",
      "Epoch 502/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0026 - val_loss: 0.2754\n",
      "Epoch 503/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0023 - val_loss: 0.2708\n",
      "Epoch 504/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0025 - val_loss: 0.2713\n",
      "Epoch 505/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0025 - val_loss: 0.2722\n",
      "Epoch 506/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0023 - val_loss: 0.2747\n",
      "Epoch 507/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0022 - val_loss: 0.2771\n",
      "Epoch 508/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0023 - val_loss: 0.2751\n",
      "Epoch 509/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0025 - val_loss: 0.2726\n",
      "Epoch 510/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0026 - val_loss: 0.2824\n",
      "Epoch 511/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0025 - val_loss: 0.2768\n",
      "Epoch 512/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0022 - val_loss: 0.2780\n",
      "Epoch 513/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0022 - val_loss: 0.2771\n",
      "Epoch 514/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0024 - val_loss: 0.2801\n",
      "Epoch 515/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0021 - val_loss: 0.2815\n",
      "Epoch 516/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0024 - val_loss: 0.2810\n",
      "Epoch 517/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0022 - val_loss: 0.2851\n",
      "Epoch 518/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0021 - val_loss: 0.2851\n",
      "Epoch 519/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0020 - val_loss: 0.2830\n",
      "Epoch 520/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0021 - val_loss: 0.2865\n",
      "Epoch 521/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0023 - val_loss: 0.2836\n",
      "Epoch 522/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0021 - val_loss: 0.2877\n",
      "Epoch 523/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0022 - val_loss: 0.2822\n",
      "Epoch 524/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0021 - val_loss: 0.2856\n",
      "Epoch 525/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0020 - val_loss: 0.2869\n",
      "Epoch 526/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0020 - val_loss: 0.2860\n",
      "Epoch 527/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0020 - val_loss: 0.2872\n",
      "Epoch 528/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0020 - val_loss: 0.2896\n",
      "Epoch 529/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0022 - val_loss: 0.2902\n",
      "Epoch 530/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0020 - val_loss: 0.2889\n",
      "Epoch 531/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0019 - val_loss: 0.2916\n",
      "Epoch 532/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0026 - val_loss: 0.2894\n",
      "Epoch 533/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0023 - val_loss: 0.2978\n",
      "Epoch 534/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0027 - val_loss: 0.2857\n",
      "Epoch 535/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0024 - val_loss: 0.3000\n",
      "Epoch 536/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0019 - val_loss: 0.2927\n",
      "Epoch 537/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0023 - val_loss: 0.3025\n",
      "Epoch 538/600\n",
      "398/398 [==============================] - 0s 99us/sample - loss: 0.0024 - val_loss: 0.2904\n",
      "Epoch 539/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0025 - val_loss: 0.2932\n",
      "Epoch 540/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0024 - val_loss: 0.2965\n",
      "Epoch 541/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0029 - val_loss: 0.2930\n",
      "Epoch 542/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0029 - val_loss: 0.3053\n",
      "Epoch 543/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0032 - val_loss: 0.2988\n",
      "Epoch 544/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0024 - val_loss: 0.3057\n",
      "Epoch 545/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0017 - val_loss: 0.2964\n",
      "Epoch 546/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0020 - val_loss: 0.3033\n",
      "Epoch 547/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0018 - val_loss: 0.2962\n",
      "Epoch 548/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0019 - val_loss: 0.2935\n",
      "Epoch 549/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0024 - val_loss: 0.3050\n",
      "Epoch 550/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0019 - val_loss: 0.3020\n",
      "Epoch 551/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0017 - val_loss: 0.3059\n",
      "Epoch 552/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0017 - val_loss: 0.3066\n",
      "Epoch 553/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0016 - val_loss: 0.3116\n",
      "Epoch 554/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0017 - val_loss: 0.3048\n",
      "Epoch 555/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0016 - val_loss: 0.3057\n",
      "Epoch 556/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0017 - val_loss: 0.3108\n",
      "Epoch 557/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0019 - val_loss: 0.3086\n",
      "Epoch 558/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0018 - val_loss: 0.3037\n",
      "Epoch 559/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0016 - val_loss: 0.3119\n",
      "Epoch 560/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0019 - val_loss: 0.3065\n",
      "Epoch 561/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0016 - val_loss: 0.3124\n",
      "Epoch 562/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0015 - val_loss: 0.3128\n",
      "Epoch 563/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0017 - val_loss: 0.3138\n",
      "Epoch 564/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0018 - val_loss: 0.3127\n",
      "Epoch 565/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0020 - val_loss: 0.3184\n",
      "Epoch 566/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0017 - val_loss: 0.3157\n",
      "Epoch 567/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0015 - val_loss: 0.3173\n",
      "Epoch 568/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0015 - val_loss: 0.3192\n",
      "Epoch 569/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0015 - val_loss: 0.3193\n",
      "Epoch 570/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0015 - val_loss: 0.3180\n",
      "Epoch 571/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0015 - val_loss: 0.3174\n",
      "Epoch 572/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0014 - val_loss: 0.3203\n",
      "Epoch 573/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0015 - val_loss: 0.3148\n",
      "Epoch 574/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0016 - val_loss: 0.3178\n",
      "Epoch 575/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0014 - val_loss: 0.3175\n",
      "Epoch 576/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0015 - val_loss: 0.3155\n",
      "Epoch 577/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0015 - val_loss: 0.3193\n",
      "Epoch 578/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0015 - val_loss: 0.3180\n",
      "Epoch 579/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0014 - val_loss: 0.3188\n",
      "Epoch 580/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0015 - val_loss: 0.3216\n",
      "Epoch 581/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0015 - val_loss: 0.3147\n",
      "Epoch 582/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0015 - val_loss: 0.3309\n",
      "Epoch 583/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0016 - val_loss: 0.3145\n",
      "Epoch 584/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0014 - val_loss: 0.3292\n",
      "Epoch 585/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0022 - val_loss: 0.3024\n",
      "Epoch 586/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0014 - val_loss: 0.3103\n",
      "Epoch 587/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0017 - val_loss: 0.2995\n",
      "Epoch 588/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0016 - val_loss: 0.3278\n",
      "Epoch 589/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0017 - val_loss: 0.3078\n",
      "Epoch 590/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0021 - val_loss: 0.3157\n",
      "Epoch 591/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0014 - val_loss: 0.3179\n",
      "Epoch 592/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0014 - val_loss: 0.3206\n",
      "Epoch 593/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0014 - val_loss: 0.3330\n",
      "Epoch 594/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0020 - val_loss: 0.3189\n",
      "Epoch 595/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0016 - val_loss: 0.3257\n",
      "Epoch 596/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0012 - val_loss: 0.3258\n",
      "Epoch 597/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0013 - val_loss: 0.3260\n",
      "Epoch 598/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0014 - val_loss: 0.3385\n",
      "Epoch 599/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0014 - val_loss: 0.3270\n",
      "Epoch 600/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0013 - val_loss: 0.3295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fadf727d590>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train, epochs=600,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acef0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training and validation loss\n",
    "loss = pd.DataFrame(model.history.history)\n",
    "loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c3c90",
   "metadata": {},
   "source": [
    "The Validation set error starts to increase at a certain number of epochs, even as training error continues to decrease. At the end of training, the validation error is higher than training, an indication of overfitting. This can be mitigated by several techniques, such as Early Stopping and Dropout Regularization. First, Early Stopping will be added to the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a404a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine model with early stopping callback \n",
    "# monitor = what we are trying to monitor (validation loss)\n",
    "# mode = what we are trying to do to our value (minimize)\n",
    "# verbose = whether or not we want a report printed (1)\n",
    "# patience = the number of epochs we wait after criteria is met before stopping (25)\n",
    "early_stop = EarlyStopping(monitor='val_loss',mode='min',verbose=1, patience=25)\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(30,activation='relu'))\n",
    "model2.add(Dense(15,activation='relu'))\n",
    "model2.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d414d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples, validate on 171 samples\n",
      "Epoch 1/600\n",
      "398/398 [==============================] - 1s 1ms/sample - loss: 0.6890 - val_loss: 0.6762\n",
      "Epoch 2/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.6609 - val_loss: 0.6500\n",
      "Epoch 3/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.6347 - val_loss: 0.6238\n",
      "Epoch 4/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.6054 - val_loss: 0.5905\n",
      "Epoch 5/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.5640 - val_loss: 0.5471\n",
      "Epoch 6/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.5176 - val_loss: 0.5023\n",
      "Epoch 7/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.4694 - val_loss: 0.4586\n",
      "Epoch 8/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.4210 - val_loss: 0.4136\n",
      "Epoch 9/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.3778 - val_loss: 0.3739\n",
      "Epoch 10/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.3404 - val_loss: 0.3377\n",
      "Epoch 11/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.3046 - val_loss: 0.3058\n",
      "Epoch 12/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.2775 - val_loss: 0.2798\n",
      "Epoch 13/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.2513 - val_loss: 0.2576\n",
      "Epoch 14/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.2314 - val_loss: 0.2404\n",
      "Epoch 15/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.2147 - val_loss: 0.2273\n",
      "Epoch 16/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.1999 - val_loss: 0.2156\n",
      "Epoch 17/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1886 - val_loss: 0.2050\n",
      "Epoch 18/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1826 - val_loss: 0.1973\n",
      "Epoch 19/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1689 - val_loss: 0.1891\n",
      "Epoch 20/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1587 - val_loss: 0.1833\n",
      "Epoch 21/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.1508 - val_loss: 0.1776\n",
      "Epoch 22/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1443 - val_loss: 0.1725\n",
      "Epoch 23/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.1355 - val_loss: 0.1679\n",
      "Epoch 24/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.1304 - val_loss: 0.1636\n",
      "Epoch 25/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1243 - val_loss: 0.1611\n",
      "Epoch 26/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1196 - val_loss: 0.1565\n",
      "Epoch 27/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1120 - val_loss: 0.1539\n",
      "Epoch 28/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.1084 - val_loss: 0.1514\n",
      "Epoch 29/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.1040 - val_loss: 0.1489\n",
      "Epoch 30/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.1016 - val_loss: 0.1469\n",
      "Epoch 31/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0968 - val_loss: 0.1445\n",
      "Epoch 32/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0925 - val_loss: 0.1432\n",
      "Epoch 33/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0885 - val_loss: 0.1413\n",
      "Epoch 34/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0853 - val_loss: 0.1397\n",
      "Epoch 35/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0831 - val_loss: 0.1388\n",
      "Epoch 36/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0799 - val_loss: 0.1374\n",
      "Epoch 37/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0781 - val_loss: 0.1368\n",
      "Epoch 38/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0773 - val_loss: 0.1351\n",
      "Epoch 39/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0744 - val_loss: 0.1350\n",
      "Epoch 40/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0719 - val_loss: 0.1333\n",
      "Epoch 41/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0687 - val_loss: 0.1333\n",
      "Epoch 42/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0686 - val_loss: 0.1322\n",
      "Epoch 43/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0681 - val_loss: 0.1327\n",
      "Epoch 44/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0674 - val_loss: 0.1317\n",
      "Epoch 45/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0643 - val_loss: 0.1322\n",
      "Epoch 46/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0634 - val_loss: 0.1316\n",
      "Epoch 47/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0609 - val_loss: 0.1311\n",
      "Epoch 48/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0597 - val_loss: 0.1308\n",
      "Epoch 49/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0592 - val_loss: 0.1304\n",
      "Epoch 50/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0569 - val_loss: 0.1316\n",
      "Epoch 51/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0565 - val_loss: 0.1303\n",
      "Epoch 52/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0556 - val_loss: 0.1306\n",
      "Epoch 53/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0545 - val_loss: 0.1303\n",
      "Epoch 54/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0554 - val_loss: 0.1306\n",
      "Epoch 55/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0545 - val_loss: 0.1301\n",
      "Epoch 56/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0543 - val_loss: 0.1305\n",
      "Epoch 57/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0573 - val_loss: 0.1307\n",
      "Epoch 58/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0539 - val_loss: 0.1314\n",
      "Epoch 59/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0517 - val_loss: 0.1302\n",
      "Epoch 60/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0498 - val_loss: 0.1309\n",
      "Epoch 61/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0507 - val_loss: 0.1303\n",
      "Epoch 62/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0489 - val_loss: 0.1312\n",
      "Epoch 63/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0486 - val_loss: 0.1299\n",
      "Epoch 64/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0478 - val_loss: 0.1308\n",
      "Epoch 65/600\n",
      "398/398 [==============================] - 0s 103us/sample - loss: 0.0481 - val_loss: 0.1310\n",
      "Epoch 66/600\n",
      "398/398 [==============================] - 0s 106us/sample - loss: 0.0469 - val_loss: 0.1310\n",
      "Epoch 67/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0489 - val_loss: 0.1314\n",
      "Epoch 68/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0473 - val_loss: 0.1312\n",
      "Epoch 69/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0476 - val_loss: 0.1311\n",
      "Epoch 70/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0453 - val_loss: 0.1314\n",
      "Epoch 71/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0458 - val_loss: 0.1314\n",
      "Epoch 72/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0454 - val_loss: 0.1317\n",
      "Epoch 73/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0447 - val_loss: 0.1306\n",
      "Epoch 74/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0441 - val_loss: 0.1307\n",
      "Epoch 75/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0433 - val_loss: 0.1320\n",
      "Epoch 76/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0432 - val_loss: 0.1329\n",
      "Epoch 77/600\n",
      "398/398 [==============================] - 0s 104us/sample - loss: 0.0431 - val_loss: 0.1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "398/398 [==============================] - 0s 105us/sample - loss: 0.0433 - val_loss: 0.1336\n",
      "Epoch 79/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0434 - val_loss: 0.1334\n",
      "Epoch 80/600\n",
      "398/398 [==============================] - 0s 100us/sample - loss: 0.0433 - val_loss: 0.1347\n",
      "Epoch 81/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0411 - val_loss: 0.1348\n",
      "Epoch 82/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0413 - val_loss: 0.1346\n",
      "Epoch 83/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0415 - val_loss: 0.1355\n",
      "Epoch 84/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0434 - val_loss: 0.1355\n",
      "Epoch 85/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0413 - val_loss: 0.1349\n",
      "Epoch 86/600\n",
      "398/398 [==============================] - 0s 101us/sample - loss: 0.0393 - val_loss: 0.1351\n",
      "Epoch 87/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0397 - val_loss: 0.1364\n",
      "Epoch 88/600\n",
      "398/398 [==============================] - 0s 102us/sample - loss: 0.0414 - val_loss: 0.1384\n",
      "Epoch 00088: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fadf76cd790>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain model using early stopping\n",
    "model2.fit(x=X_train,y=y_train,epochs=600, validation_data=(X_test,y_test),callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "575001cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses with early stopping\n",
    "loss = pd.DataFrame(model2.history.history)\n",
    "loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a5a28d",
   "metadata": {},
   "source": [
    "There was a noticeable improvement with Early Stopping. The Validation loss does not increase at all, so overfitting is reduced. There is still a noticeable difference between the validation and training loss, which can be further reduced through other regularization techniques, such as Dropout Regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8be11d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain model with dropout regularization\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(30,activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(15,activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bacf8465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples, validate on 171 samples\n",
      "Epoch 1/600\n",
      "398/398 [==============================] - 1s 1ms/sample - loss: 0.6911 - val_loss: 0.6772\n",
      "Epoch 2/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.6758 - val_loss: 0.6641\n",
      "Epoch 3/600\n",
      "398/398 [==============================] - 0s 116us/sample - loss: 0.6600 - val_loss: 0.6520\n",
      "Epoch 4/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.6398 - val_loss: 0.6327\n",
      "Epoch 5/600\n",
      "398/398 [==============================] - 0s 115us/sample - loss: 0.6304 - val_loss: 0.6122\n",
      "Epoch 6/600\n",
      "398/398 [==============================] - 0s 114us/sample - loss: 0.6086 - val_loss: 0.5869\n",
      "Epoch 7/600\n",
      "398/398 [==============================] - 0s 114us/sample - loss: 0.5970 - val_loss: 0.5540\n",
      "Epoch 8/600\n",
      "398/398 [==============================] - 0s 114us/sample - loss: 0.5800 - val_loss: 0.5183\n",
      "Epoch 9/600\n",
      "398/398 [==============================] - 0s 115us/sample - loss: 0.5427 - val_loss: 0.4825\n",
      "Epoch 10/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.5320 - val_loss: 0.4529\n",
      "Epoch 11/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.5091 - val_loss: 0.4255\n",
      "Epoch 12/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.4819 - val_loss: 0.3968\n",
      "Epoch 13/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.4466 - val_loss: 0.3714\n",
      "Epoch 14/600\n",
      "398/398 [==============================] - 0s 114us/sample - loss: 0.4345 - val_loss: 0.3505\n",
      "Epoch 15/600\n",
      "398/398 [==============================] - 0s 114us/sample - loss: 0.3956 - val_loss: 0.3303\n",
      "Epoch 16/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.4201 - val_loss: 0.3136\n",
      "Epoch 17/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.3916 - val_loss: 0.3005\n",
      "Epoch 18/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.3673 - val_loss: 0.2833\n",
      "Epoch 19/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.3607 - val_loss: 0.2644\n",
      "Epoch 20/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.3353 - val_loss: 0.2531\n",
      "Epoch 21/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.3295 - val_loss: 0.2415\n",
      "Epoch 22/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.3150 - val_loss: 0.2324\n",
      "Epoch 23/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.2798 - val_loss: 0.2214\n",
      "Epoch 24/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.2926 - val_loss: 0.2104\n",
      "Epoch 25/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.2911 - val_loss: 0.2072\n",
      "Epoch 26/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.2824 - val_loss: 0.2008\n",
      "Epoch 27/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.2589 - val_loss: 0.1927\n",
      "Epoch 28/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.2692 - val_loss: 0.1864\n",
      "Epoch 29/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.2592 - val_loss: 0.1814\n",
      "Epoch 30/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.2477 - val_loss: 0.1853\n",
      "Epoch 31/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.2267 - val_loss: 0.1761\n",
      "Epoch 32/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.2323 - val_loss: 0.1713\n",
      "Epoch 33/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.2352 - val_loss: 0.1670\n",
      "Epoch 34/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.2332 - val_loss: 0.1654\n",
      "Epoch 35/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.2055 - val_loss: 0.1643\n",
      "Epoch 36/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.2117 - val_loss: 0.1552\n",
      "Epoch 37/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.2098 - val_loss: 0.1514\n",
      "Epoch 38/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1982 - val_loss: 0.1485\n",
      "Epoch 39/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.2056 - val_loss: 0.1455\n",
      "Epoch 40/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1893 - val_loss: 0.1440\n",
      "Epoch 41/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1792 - val_loss: 0.1441\n",
      "Epoch 42/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1689 - val_loss: 0.1394\n",
      "Epoch 43/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1829 - val_loss: 0.1375\n",
      "Epoch 44/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1801 - val_loss: 0.1384\n",
      "Epoch 45/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1558 - val_loss: 0.1358\n",
      "Epoch 46/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1691 - val_loss: 0.1357\n",
      "Epoch 47/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1655 - val_loss: 0.1317\n",
      "Epoch 48/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1674 - val_loss: 0.1302\n",
      "Epoch 49/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1633 - val_loss: 0.1319\n",
      "Epoch 50/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1465 - val_loss: 0.1275\n",
      "Epoch 51/600\n",
      "398/398 [==============================] - 0s 114us/sample - loss: 0.1310 - val_loss: 0.1260\n",
      "Epoch 52/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1340 - val_loss: 0.1244\n",
      "Epoch 53/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1154 - val_loss: 0.1232\n",
      "Epoch 54/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1661 - val_loss: 0.1229\n",
      "Epoch 55/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1653 - val_loss: 0.1265\n",
      "Epoch 56/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1450 - val_loss: 0.1205\n",
      "Epoch 57/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1405 - val_loss: 0.1175\n",
      "Epoch 58/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1410 - val_loss: 0.1258\n",
      "Epoch 59/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1172 - val_loss: 0.1211\n",
      "Epoch 60/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.1205 - val_loss: 0.1149\n",
      "Epoch 61/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1630 - val_loss: 0.1162\n",
      "Epoch 62/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1301 - val_loss: 0.1160\n",
      "Epoch 63/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1258 - val_loss: 0.1133\n",
      "Epoch 64/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1381 - val_loss: 0.1189\n",
      "Epoch 65/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1248 - val_loss: 0.1120\n",
      "Epoch 66/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1277 - val_loss: 0.1177\n",
      "Epoch 67/600\n",
      "398/398 [==============================] - 0s 114us/sample - loss: 0.1260 - val_loss: 0.1100\n",
      "Epoch 68/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1310 - val_loss: 0.1085\n",
      "Epoch 69/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.1415 - val_loss: 0.1101\n",
      "Epoch 70/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1255 - val_loss: 0.1093\n",
      "Epoch 71/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1158 - val_loss: 0.1143\n",
      "Epoch 72/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.1062 - val_loss: 0.1084\n",
      "Epoch 73/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1186 - val_loss: 0.1091\n",
      "Epoch 74/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1200 - val_loss: 0.1096\n",
      "Epoch 75/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.1135 - val_loss: 0.1099\n",
      "Epoch 76/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.1054 - val_loss: 0.1084\n",
      "Epoch 77/600\n",
      "398/398 [==============================] - 0s 112us/sample - loss: 0.0897 - val_loss: 0.1081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.0976 - val_loss: 0.1117\n",
      "Epoch 79/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0771 - val_loss: 0.1075\n",
      "Epoch 80/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.1114 - val_loss: 0.1120\n",
      "Epoch 81/600\n",
      "398/398 [==============================] - 0s 107us/sample - loss: 0.0837 - val_loss: 0.1096\n",
      "Epoch 82/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.1056 - val_loss: 0.1052\n",
      "Epoch 83/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0948 - val_loss: 0.1050\n",
      "Epoch 84/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.1224 - val_loss: 0.1116\n",
      "Epoch 85/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.1132 - val_loss: 0.1038\n",
      "Epoch 86/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.1047 - val_loss: 0.1030\n",
      "Epoch 87/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.0850 - val_loss: 0.1106\n",
      "Epoch 88/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0943 - val_loss: 0.1084\n",
      "Epoch 89/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.1035 - val_loss: 0.1019\n",
      "Epoch 90/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0884 - val_loss: 0.1010\n",
      "Epoch 91/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0890 - val_loss: 0.1016\n",
      "Epoch 92/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0879 - val_loss: 0.1036\n",
      "Epoch 93/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0979 - val_loss: 0.1041\n",
      "Epoch 94/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0975 - val_loss: 0.1157\n",
      "Epoch 95/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.0819 - val_loss: 0.1051\n",
      "Epoch 96/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0906 - val_loss: 0.1049\n",
      "Epoch 97/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0969 - val_loss: 0.1082\n",
      "Epoch 98/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.1075 - val_loss: 0.1083\n",
      "Epoch 99/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0922 - val_loss: 0.1051\n",
      "Epoch 100/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.0970 - val_loss: 0.1073\n",
      "Epoch 101/600\n",
      "398/398 [==============================] - 0s 108us/sample - loss: 0.0768 - val_loss: 0.1069\n",
      "Epoch 102/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.1053 - val_loss: 0.1053\n",
      "Epoch 103/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0911 - val_loss: 0.1023\n",
      "Epoch 104/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0878 - val_loss: 0.1067\n",
      "Epoch 105/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0854 - val_loss: 0.1211\n",
      "Epoch 106/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.0906 - val_loss: 0.1035\n",
      "Epoch 107/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0884 - val_loss: 0.1056\n",
      "Epoch 108/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0677 - val_loss: 0.1133\n",
      "Epoch 109/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0662 - val_loss: 0.1081\n",
      "Epoch 110/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0784 - val_loss: 0.1144\n",
      "Epoch 111/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0924 - val_loss: 0.1060\n",
      "Epoch 112/600\n",
      "398/398 [==============================] - 0s 113us/sample - loss: 0.0766 - val_loss: 0.1036\n",
      "Epoch 113/600\n",
      "398/398 [==============================] - 0s 109us/sample - loss: 0.0825 - val_loss: 0.1057\n",
      "Epoch 114/600\n",
      "398/398 [==============================] - 0s 111us/sample - loss: 0.0681 - val_loss: 0.1064\n",
      "Epoch 115/600\n",
      "398/398 [==============================] - 0s 110us/sample - loss: 0.0932 - val_loss: 0.1065\n",
      "Epoch 00115: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fadd1526a50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x=X_train, y=y_train, epochs=600,validation_data=(X_test,y_test),callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75a90653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the losses\n",
    "loss = pd.DataFrame(model3.history.history)\n",
    "loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc83ee",
   "metadata": {},
   "source": [
    "By adding dropout regularization to the model, validation loss is now much closer to training loss. Overfitting has been minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6147c8b",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4556bf",
   "metadata": {},
   "source": [
    "For classification problems, the confusion matrix will useful as well as the accuracy metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d245ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model to predict on test data \n",
    "predictions = model3.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b066d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103   2]\n",
      " [  3  63]]\n"
     ]
    }
   ],
   "source": [
    "# generate confusion matrix and accuracy score \n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fd57432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       105\n",
      "           1       0.97      0.95      0.96        66\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.97      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06710c",
   "metadata": {},
   "source": [
    "The final model achieved an accuracy of 97% at classifying tumors as benign or malignant. Early Stopping and Dropout Regularization proved to be effective tools at minimizing overfitting. Since the test set was used as validation during Early Stopping, the final evaluation of the model is biased. To get an unbiased evaluation of this model, an actual validation set needs to be utilized. Additionally, model hyperparameters should be tuned on the validation set before evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f5267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
